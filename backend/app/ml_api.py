# backend/app/ml_api.py

import requests

# Version 3
def get_ml_response(user_input: str) -> str:
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3",  # –∏–ª–∏ llama2
                "prompt": user_input,
                "stream": False
            },
            timeout=20
        )

        print("Ollama –æ—Ç–≤–µ—Ç:", response.json())  # üëà –ª–æ–≥ –≤ –∫–æ–Ω—Å–æ–ª—å
        return response.json().get("response", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞").strip()

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: {e}"



# Version 2
# def get_ml_response(user_input: str) -> str:
#     try:
#         response = requests.post(
#             "http://127.0.0.1:8001/predict",
#             json={"message": user_input},
#             timeout=5
#         )
#         return response.json().get("answer", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç ML API")
#     except Exception as e:
#         return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ ML API: {e}"

# Version 1
# –¢—É—Ç –ø—Ä–∏–º–µ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –±–µ–∑ –∑–∞–ø—Ä–æ—Å–∞ –∫ ML 
# from fastapi import FastAPI
# def get_ml_response(user_input: str) -> str:
#     # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ –∫ LLM
#     return f"Echo: {user_input}"

