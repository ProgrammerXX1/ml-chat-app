from rak_ml.file_reader import read_file
from rak_ml.chunking import chunk_text
from rak_ml.indexer import embed_and_store
from rak_ml.retriever import retrieve_relevant_chunks
from rak_ml.qa_chain import generate_answer
from rak_ml.llm import AVAILABLE_MODELS

def process_document(filepath: str, namespace: str = "DocumentChunk") -> int:
    """
    Ğ§Ñ‚ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ñ‡Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ² Weaviate.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‡Ğ°Ğ½ĞºĞ¾Ğ².
    """
    text = read_file(filepath)
    chunks = chunk_text(text)
    embed_and_store(chunks, namespace)
    return len(chunks)

def get_available_models() -> list:
    """
    Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· AVAILABLE_MODELS.
    """
    return list(AVAILABLE_MODELS.keys())

def answer_query(question: str, model_name: str = "llama3:latest", filepath: str = None) -> str:
    if filepath:
        print(f"ğŸ“„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ñ„Ğ°Ğ¹Ğ»: {filepath}")
        text = read_file(filepath)
        chunks = chunk_text(text)
        print(f"ğŸ“¥ Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ğ² Weaviate...")
        embed_and_store(chunks)

    relevant_chunks = retrieve_relevant_chunks(question)
    print(f"ğŸ“š ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²: {len(relevant_chunks)}")
    answer = generate_answer(question, relevant_chunks, model_name)
    return answer