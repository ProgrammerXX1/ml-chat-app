import subprocess
import json
import requests
# ‚úÖ –ö–ª–∞—Å—Å OllamaProvider —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
class OllamaProvider:
    def __init__(self, model: str = "llama3", server_url="http://192.168.0.50:11434"):
        self.model = model
        self.server_url = server_url

    def chat(self, prompt: str) -> str:
        try:
            response = requests.post(
                f"{self.server_url}/api/generate",
                json={"model": self.model, "prompt": prompt},
                timeout=30
            )
            response.raise_for_status()
            return response.json()["response"]
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–µ—Ä–≤–µ—Ä—É –º–æ–¥–µ–ª–∏: {e}"

# üß© –î–æ–ø—É—Å—Ç–∏–º—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∑–∞–≥—Ä—É–∑–∏–ª
AVAILABLE_MODELS = {
    "llama3 18gb": "llama3:latest",
    "mistral 4.1gb": "mistral:latest",
    "qwen7b 4.5gb": "qwen:7b",
    "qwen14b 8.2gb": "qwen:14b",
    "gemma2b 1.7gb": "gemma:2b",
    "deepseek6.7b 3.8gb": "deepseek-coder:6.7b",
    "deepseek33b 18 gb": "deepseek-coder:33b",
    "deepseekLite 3.8gb": "deepseek-coder:latest",
    "phi 1.6gb":"phi:latest",
    "gamma:2b 1.7gb":"gamma:2b"
}

# üîß –î—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã (–∑–∞–≥–ª—É—à–∫–∏)
class OpenAIProvider:
    def chat(self, prompt: str) -> str:
        return "üîß OpenAIProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

class GroqProvider:
    def chat(self, prompt: str) -> str:
        return "üîß GroqProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

# üîå –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø
def get_llm(provider_name: str):
    # –ï—Å–ª–∏ —ç—Ç–æ –∫–ª—é—á –∏–∑ AVAILABLE_MODELS ‚Üí –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è
    model = AVAILABLE_MODELS.get(provider_name, provider_name)

    if model in ollama_installed_models():
        return OllamaProvider(model)

    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–ª–∏ –º–æ–¥–µ–ª—å: {provider_name}")
def ollama_installed_models():
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
    return [line.split()[0] for line in result.stdout.strip().splitlines()[1:]]
