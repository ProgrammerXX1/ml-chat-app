import subprocess
import json
import requests

import os
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
load_dotenv()

# –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ä–µ–¥—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
OLLAMA_SERVER_URL = os.getenv("OLLAMA_SERVER_URL", "http://localhost:11434")

class OllamaProvider:
    def __init__(self, model: str = "llama3", server_url=OLLAMA_SERVER_URL):
        self.model = model
        self.server_url = server_url

    def chat(self, prompt: str) -> str:
        try:
            response = requests.post(
                f"{self.server_url}/api/generate",
                json={"model": self.model, "prompt": prompt},
                timeout=60,
                stream=True  # üß† –≤–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä!
            )
            response.raise_for_status()

            full_text = ""
            for line in response.iter_lines(decode_unicode=True):
                if line:
                    try:
                        chunk = json.loads(line)
                        full_text += chunk.get("response", "")
                    except json.JSONDecodeError:
                        continue  # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –º—É—Å–æ—Ä

            return full_text or "‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"

        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–µ—Ä–≤–µ—Ä—É –º–æ–¥–µ–ª–∏: {e}"



# üß© –î–æ–ø—É—Å—Ç–∏–º—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∑–∞–≥—Ä—É–∑–∏–ª
AVAILABLE_MODELS = {
    "llama3 18gb": "llama3:latest",
    "mistral 4.1gb": "mistral:latest",
    "qwen7b 4.5gb": "qwen:7b",
    "qwen14b 8.2gb": "qwen:14b",
    "gemma2b 1.7gb": "gemma:2b",
    "deepseek6.7b 3.8gb": "deepseek-coder:6.7b",
    "deepseek33b 18 gb": "deepseek-coder:33b",
    "deepseekLite 3.8gb": "deepseek-coder:latest",
    "phi 1.6gb":"phi:latest",
    "gamma:2b 1.7gb":"gamma:2b"
}

# üîß –î—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã (–∑–∞–≥–ª—É—à–∫–∏)
class OpenAIProvider:
    def chat(self, prompt: str) -> str:
        return "üîß OpenAIProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

class GroqProvider:
    def chat(self, prompt: str) -> str:
        return "üîß GroqProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

# üîå –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø
def get_llm(provider_name: str):
    # –ï—Å–ª–∏ —ç—Ç–æ –∫–ª—é—á –∏–∑ AVAILABLE_MODELS ‚Üí –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è
    model = AVAILABLE_MODELS.get(provider_name, provider_name)

    if model in ollama_installed_models():
        return OllamaProvider(model)

    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–ª–∏ –º–æ–¥–µ–ª—å: {provider_name}")
def ollama_installed_models():
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
        lines = result.stdout.strip().splitlines()
        if len(lines) <= 1:
            return []
        return [line.split()[0] for line in lines[1:]]
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ `ollama list`: {e}")
        return []