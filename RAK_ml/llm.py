import subprocess
import json

# ‚úÖ –ö–ª–∞—Å—Å OllamaProvider —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
class OllamaProvider:
    def __init__(self, model: str = "llama3:latest"):
        self.model = model

    def chat(self, prompt: str) -> str:
        try:
            result = subprocess.run(
                ["ollama", "run", self.model],
                input=prompt.encode("utf-8"),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=180
            )
            if result.returncode != 0:
                return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ollama run: {result.stderr.decode('utf-8')}"
            return result.stdout.decode("utf-8").strip()
        except subprocess.TimeoutExpired:
            return "‚ö†Ô∏è –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama –∏—Å—Ç–µ–∫–ª–æ."

# üß© –î–æ–ø—É—Å—Ç–∏–º—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∑–∞–≥—Ä—É–∑–∏–ª
AVAILABLE_MODELS = {
    "llama3": "llama3:latest",
    "mistral": "mistral:latest",
    "qwen7b": "qwen:7b",
    "qwen14b": "qwen:14b",
    "gemma2b": "gemma:2b",
    "deepseek6.7b": "deepseek-coder:6.7b",
    "deepseek33b": "deepseek-coder:33b",
    "deepseekLite": "deepseek-coder:latest"
}

# üîß –î—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã (–∑–∞–≥–ª—É—à–∫–∏)
class OpenAIProvider:
    def chat(self, prompt: str) -> str:
        return "üîß OpenAIProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

class GroqProvider:
    def chat(self, prompt: str) -> str:
        return "üîß GroqProvider –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Ollama."

# üîå –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø
def get_llm(provider_name: str):
    # –ï—Å–ª–∏ —ç—Ç–æ –∫–ª—é—á –∏–∑ AVAILABLE_MODELS ‚Üí –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è
    model = AVAILABLE_MODELS.get(provider_name, provider_name)

    if model in ollama_installed_models():
        return OllamaProvider(model)

    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –∏–ª–∏ –º–æ–¥–µ–ª—å: {provider_name}")
def ollama_installed_models():
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
    return [line.split()[0] for line in result.stdout.strip().splitlines()[1:]]
