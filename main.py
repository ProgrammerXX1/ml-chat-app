import subprocess
from RAK_ml.file_reader import read_file
from RAK_ml.chunking import chunk_text
from RAK_ml.indexer import embed_and_store
from RAK_ml.retriever import retrieve_relevant_chunks
from RAK_ml.qa_chain import generate_answer

def list_ollama_models():
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
    lines = result.stdout.strip().splitlines()[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    models = [line.split()[0] for line in lines]
    return models

def select_model(models):
    print("\nüì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (ollama list):")
    for i, name in enumerate(models, start=1):
        print(f"{i}. {name}")
    print()
    while True:
        try:
            choice = int(input("–í—ã–±–µ—Ä–∏ –Ω–æ–º–µ—Ä –º–æ–¥–µ–ª–∏ ‚Üí "))
            if 1 <= choice <= len(models):
                return models[choice - 1]
        except ValueError:
            pass
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë.")

def main():
    filepath = "example.txt"  # ‚Üê –ó–∞–º–µ–Ω–∏—Ç—å –ø—É—Ç—å –≤—Ä—É—á–Ω—É—é –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª: {filepath}")
    text = read_file(filepath)

    print("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = chunk_text(text)
    print(f"üî¢ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

    print("üì• –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Weaviate...")
    embed_and_store(chunks)

    models = list_ollama_models()
    model = select_model(models)
    print(f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model}\n")

    while True:
        question = input("üß† –í–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        if question.lower() in {"exit", "–≤—ã—Ö–æ–¥"}:
            break
        if question.lower() == "/model":
            model = select_model(models)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model}")
            continue
        relevant_chunks = retrieve_relevant_chunks(question)
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {len(relevant_chunks)}")
        answer = generate_answer(question, relevant_chunks, model)
        print(f"\nü§ñ –û—Ç–≤–µ—Ç:\n{answer}\n")

if __name__ == "__main__":
    main()
